{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider the following table which details the use of contraception by age of currently married women in El Salvador in 1985.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th colspan=\"3\">Contraceptive Method</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Age</b></td>\n",
    "    <td><b>Ster.</b></td>\n",
    "    <td><b>Other</b></td>\n",
    "    <td><b>None</b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15 - 19</td>\n",
    "    <td>3</td>\n",
    "    <td>61</td>\n",
    "    <td>232</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20 - 24</td>\n",
    "    <td>80</td>\n",
    "    <td>137</td>\n",
    "    <td>400</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25 - 29</td>\n",
    "    <td>216</td>\n",
    "    <td>131</td>\n",
    "    <td>301</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30 - 34</td>\n",
    "    <td>268</td>\n",
    "    <td>76</td>\n",
    "    <td>203</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35 - 39</td>\n",
    "    <td>197</td>\n",
    "    <td>50</td>\n",
    "    <td>188</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>40 - 44</td>\n",
    "    <td>150</td>\n",
    "    <td>24</td>\n",
    "    <td>164</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>45 - 49</td>\n",
    "    <td>91</td>\n",
    "    <td>10</td>\n",
    "    <td>183</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Let $ t_{i} $ denote an indicator variable which takes on values $ \\{1,2,3\\} $ where 1 is Sterilization, 2 is Other, and 3 is None. Consider the activation function:\n",
    "\n",
    "$$ a_{ij}=\\alpha_{j}+\\beta_{j}x_{i}+\\gamma_{j}x_{i}^{2} $$\n",
    "\n",
    "#### where $ x_{i} $ is the mid-point of the $ i $-th age group, and $ j \\in \\{1,2,3\\} $. Recall that the multinomial logit model is given by\n",
    "\n",
    "$$ p\\left(t_{i}=j|x_{i}\\right)=\\frac{exp\\left(a_{ij}\\right)}{\\sum_{j'}exp\\left(a_{ij'}\\right)} $$\n",
    "\n",
    "#### Implement multinomial logistic regression from scratch (for the underlying optimization you may use modules such as the ones available in $ \\displaystyle{scipy} $) and apply it to the given data. Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [17, 22, 27, 32, 37, 42, 47]\n",
    "\n",
    "M = np.matrix([[  3,  62, 232],\n",
    "               [ 80, 137, 400],\n",
    "               [216, 131, 301],\n",
    "               [268,  76, 203],\n",
    "               [197,  50, 188],\n",
    "               [150,  24, 164],\n",
    "               [ 91,  10, 183]])\n",
    "\n",
    "def m(i, j):\n",
    "    return M[i - 1, j - 1]\n",
    "\n",
    "def activation(w, i, j):\n",
    "    s_idx = (j - 1) * 3\n",
    "    return w[s_idx] + w[s_idx + 1] * x[i - 1] + w[s_idx + 2] * x[i - 1]**2\n",
    "\n",
    "c_logit_sum = {}\n",
    "\n",
    "def logit(w, i, j):\n",
    "    key = str(w) + '_' + str(i)\n",
    "    if key not in c_logit_sum:\n",
    "        c_logit_sum[key] = np.exp(activation(w, i, 1)) + np.exp(activation(w, i, 2)) + np.exp(activation(w, i, 3))\n",
    "    sum = c_logit_sum[key]\n",
    "    return np.exp(activation(w, i, j)) / sum\n",
    "\n",
    "def error(w, i):\n",
    "    return - m(i, 1) * np.log(logit(w, i, 1)) - m(i, 2) * np.log(logit(w, i, 2)) - m(i, 3) * np.log(logit(w, i, 3))\n",
    "\n",
    "w0 = [100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "# w_ml = []\n",
    "# for s in range(len(x)):\n",
    "#     res = minimize(neg_likelihood, w0, args=(s + 1, ), method='COBYLA')\n",
    "#     if (res.success):\n",
    "#         w_ml.append(res.x)\n",
    "\n",
    "# print(w_ml)\n",
    "\n",
    "res = minimize(error, w0, args=(4, ), method='COBYLA')\n",
    "if (res.success):\n",
    "    print(res.x)\n",
    "else:\n",
    "    print(res)\n",
    "\n",
    "\n",
    "# w1_ml = [-4.30243814e+00, -7.60004802e-04,  1.21589921e-07,\n",
    "#         -1.29240916e+00,  3.29655268e-04,  1.35854539e-08,\n",
    "#          3.28688361e-02, -3.78431790e-07, -2.09425802e-07]\n",
    "\n",
    "# print(logit(w_ml, 5, 1))\n",
    "# print(logit(w_ml, 5, 2))\n",
    "# print(logit(w_ml, 5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Enron-spam dataset from http://www.aueb.gr/users/ion/data/enron-spam/. Use the pre-processed datasets Enron1, . . . , Enron5 as training data and Enron6 as test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the Naive Bayes algorithm that we discussed in the class (a good in-depth reference is http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf). Remember to perform all calculations on the log scale to prevent underow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Report the accuracy on the test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do you account for di\u000b",
    "erent prior probabilities for spam and ham?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Does the performance of the classi\f",
    "er change when you do add one Laplace smoothing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What are the most discriminative words as per the naive Bayes classifer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract all messages from archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import glob\n",
    "\n",
    "WORKDIR = 'workdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Enron-Spam/enron6.tar.gz\n",
      "Extracting Enron-Spam/enron5.tar.gz\n",
      "Extracting Enron-Spam/enron2.tar.gz\n",
      "Extracting Enron-Spam/enron1.tar.gz\n",
      "Extracting Enron-Spam/enron4.tar.gz\n",
      "Extracting Enron-Spam/enron3.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def msg_dir(members):\n",
    "    for tarinfo in members:\n",
    "        if (tarinfo.name.count('/') > 1):\n",
    "            yield tarinfo\n",
    "\n",
    "for file in glob.glob(\"Enron-Spam/*.tar.gz\"):\n",
    "    print(\"Extracting \" + file)\n",
    "    with tarfile.open(file, 'r:gz') as tar:\n",
    "        tar.extractall(path=WORKDIR, members=msg_dir(tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, start to train the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) Define a function which strip meaningless words, which includes punctuation mark and stopwords. We get the \"Default English stopwords\" from http://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "punc_regex = re.compile('[%s]' % re.escape(punctuation.replace(\"'\", \"\")))\n",
    "apos_regex = re.compile('(\\w)\\s\\'\\s(\\w)')\n",
    "\n",
    "STOPWORDS_FILENAME = 'stopwords.txt'\n",
    "with open(STOPWORDS_FILENAME, 'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# strip meaningless words from a string and return a\n",
    "# meaningful list of words.\n",
    "def strip_meaningless(s):\n",
    "    # replace any punctuation marks but the apostrophe with space\n",
    "    clean_s = punc_regex.sub(' ', s)\n",
    "    \n",
    "    # format case \"what ' s\" to \"what's\" in order to capture the stopwrods like \"that's\"\n",
    "    clean_s = apos_regex.sub('\\\\1\\'\\\\2', clean_s)\n",
    "    \n",
    "    c_l = [word.lower() for word in clean_s.split()]\n",
    "    return [word for word in c_l if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  2) Extract all vocabulary and count the number of message in class ham and spam, respectively, in this round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hams = 0\n",
    "num_spams = 0\n",
    "\n",
    "words_ham = []\n",
    "words_spam = []\n",
    "\n",
    "FILE_ENCODING = 'ISO-8859-1'\n",
    "\n",
    "# As the question mentioned, use the pre-processed datasets\n",
    "# Enron1, ..., Enron5 as training data and Enron6 as test data.\n",
    "for file in glob.glob(WORKDIR + '/enron[1-5]/*/*.txt'):\n",
    "    is_ham = file.endswith('ham.txt')\n",
    "    if is_ham:\n",
    "        num_hams += 1\n",
    "    else:\n",
    "        num_spams += 1\n",
    "\n",
    "    with open(file, 'r', encoding=FILE_ENCODING) as f_msg:\n",
    "        content = strip_meaningless(f_msg.read())\n",
    "        \n",
    "        if is_ham:\n",
    "            words_ham.extend(content)\n",
    "        else:\n",
    "            words_spam.extend(content)\n",
    "\n",
    "vocabulary = set(words_ham + words_spam)\n",
    "vocabulary.remove(\"'\")    # we think the apostrophe should not be one of word in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save all the vocabulary to vocabulary.txt if you want to have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('out/vocabulary.txt', mode='w', encoding=FILE_ENCODING) as voc_file:\n",
    "    voc_file.write('\\n'.join(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3) Compute $ \\hat{P}\\left(c\\right) $ and all $ \\hat{P}\\left(t_{k}|c\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities for ham: 0.5428272477991052\n",
      "Prior probabilities for spam: 0.4571727522008948\n",
      "The most discriminative words in condprob_ham: ('enron', 0.016913688602462673)\n",
      "The most discriminative words in condprob_spam: ('subject', 0.007590890505477282)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "prior_ham = num_hams * 1.0 / (num_hams + num_spams)\n",
    "prior_spam = num_spams * 1.0 / (num_hams + num_spams)\n",
    "\n",
    "print(\"Prior probabilities for ham: \" + str(prior_ham))\n",
    "print(\"Prior probabilities for spam: \" + str(prior_spam))\n",
    "\n",
    "words_ham_count = {}\n",
    "for word in words_ham:\n",
    "    if word in words_ham_count:\n",
    "        words_ham_count[word] += 1\n",
    "    else:\n",
    "        words_ham_count[word] = 1\n",
    "\n",
    "words_spam_count = {}\n",
    "for word in words_spam:\n",
    "    if word in words_spam_count:\n",
    "        words_spam_count[word] += 1\n",
    "    else:\n",
    "        words_spam_count[word] = 1\n",
    "\n",
    "condprob_ham = {}\n",
    "condprob_spam = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    condprob_ham[word] = (words_ham_count.get(word, 0) + 1) * 1.0 / (len(words_ham) + len(vocabulary))\n",
    "    condprob_spam[word] = (words_spam_count.get(word, 0) + 1) * 1.0 / (len(words_spam) + len(vocabulary))\n",
    "    \n",
    "    # without Laplace smoothing\n",
    "#     condprob_ham[word] = words_ham_count.get(word, 0) * 1.0 / len(words_ham)\n",
    "#     condprob_spam[word] = words_spam_count.get(word, 0) * 1.0 / len(words_spam)\n",
    "\n",
    "# sort condprob_ham and condprob_spam by values\n",
    "sorted_condprob_ham = sorted(condprob_ham.items(), key=operator.itemgetter(1))\n",
    "sorted_condprob_spam = sorted(condprob_spam.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(\"The most discriminative words in condprob_ham: \" + str(sorted_condprob_ham[-1]))\n",
    "print(\"The most discriminative words in condprob_spam: \" + str(sorted_condprob_spam[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4) Now we can apply this Naive Bayes classifier to do prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is: 0.9818333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corrects  = 0\n",
    "num_test_msgs = 0\n",
    "\n",
    "for file in glob.glob(WORKDIR + '/enron6/*/*.txt'):\n",
    "    is_true_ham = file.endswith('ham.txt')\n",
    "    \n",
    "    with open(file, 'r', encoding=FILE_ENCODING) as f_msg:\n",
    "        content = strip_meaningless(f_msg.read())\n",
    "        \n",
    "        score_ham = np.log(prior_ham)\n",
    "        score_spam = np.log(prior_spam)\n",
    "        for word in content:\n",
    "            if word in vocabulary:    # ignore word if it's not in the vocabulary\n",
    "                score_ham += np.log(condprob_ham[word])\n",
    "                score_spam += np.log(condprob_spam[word])\n",
    "            \n",
    "        num_test_msgs += 1\n",
    "        \n",
    "        if ((score_ham > score_spam) and is_true_ham) or ((score_ham < score_spam) and not is_true_ham):\n",
    "            corrects += 1\n",
    "\n",
    "print(\"The accuracy on the test set is: \" + str(corrects  * 1.0 / num_test_msgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we clean all temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(WORKDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on the same dataset as the above problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Implement the binary logistic regression algorithm that we discussed in the class. You may use any optimizer of your choice including optimization modules from $ \\displaystyle{scipy} $.**\n",
    " - **Report accuracy on test set.**\n",
    " - **How do you account for different prior probabilities for spam and ham?**\n",
    " - **What are the most discriminative words as per the logistic regression classifer?**\n",
    "\n",
    "- **Implement the Bayesian logistic regression algorithm that we discussed in the class.**\n",
    " - **Report accuracy on test set.**\n",
    " - **How does the accuracy change when the strength of the prior changes?**\n",
    "\n",
    "#### Comment on the differences and similarities between the three classifers that you implemented to solve the spam detection problem (naive bayes, binary logistic regression and bayesian logistic regression)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
